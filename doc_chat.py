# pip install streamlit==1.39.0
# pip install toml
# pip install requests
# pip install langchain
# pip install langchain_community
# pip install langchain_chroma
# pip install openai
import streamlit as st
import tempfile
import os
import requests
from openai import OpenAI
from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_community.document_loaders import TextLoader
from langchain_core.embeddings import Embeddings
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.agents import create_react_agent, AgentExecutor
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.runnables import Runnable
from langchain.tools.retriever import create_retriever_tool
import chromadb
from chromadb.config import Settings
import json
from datetime import datetime
import psutil  # éœ€è¦å®‰è£…: pip install psutil
import traceback  # ç”¨äºè¯¦ç»†é”™è¯¯è¿½è¸ª
from typing import List, Optional
from langchain_core.documents import Document

# æ·»åŠ æ—¥å¿—è®°å½•åŠŸèƒ½
def log_error(title, error):
    """è®°å½•é”™è¯¯ä¿¡æ¯åˆ°UIå’Œæ—¥å¿—"""
    error_msg = f"{title}: {str(error)}"
    st.error(error_msg)
    print(f"ERROR: {error_msg}")
    
    # è®°å½•è¯¦ç»†å †æ ˆè·Ÿè¸ª
    trace = traceback.format_exc()
    print(f"TRACE: {trace}")
    
    # åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥æ˜¾ç¤ºå †æ ˆè·Ÿè¸ª
    with st.expander("é”™è¯¯è¯¦æƒ…"):
        st.code(trace)

# æ–‡æ¡£é—®ç­”åº”ç”¨ç¨‹åº
# ä¸»è¦åŠŸèƒ½ï¼šæ”¯æŒå¤šæ–‡æ¡£ä¸Šä¼ ã€è¯­ä¹‰æ£€ç´¢ã€åŸºäºLLMçš„é—®ç­”

# å¯¼å…¥å¿…è¦çš„åº“
# æä¾›Webåº”ç”¨äº¤äº’ã€æ–‡ä»¶å¤„ç†ã€ç½‘ç»œè¯·æ±‚ã€AIæ¨¡å‹äº¤äº’ç­‰åŠŸèƒ½

# é…ç½®Streamlitåº”ç”¨é¡µé¢
# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå¸ƒå±€ï¼Œæä¾›è‰¯å¥½çš„ç”¨æˆ·ä½“éªŒ
st.set_page_config(page_title="æ–‡æ¡£é—®ç­”", layout="wide")
st.title("æ–‡æ¡£é—®ç­”")  # æ˜¾ç¤ºåº”ç”¨æ ‡é¢˜

# è‡ªå®šä¹‰åµŒå…¥ç±»ï¼šç¡…åŸºæµåŠ¨åµŒå…¥
# å°è£…æ–‡æœ¬åµŒå…¥APIè°ƒç”¨ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
class SiliconFlowEmbeddings(Embeddings):
    """ç¡…åŸºæµåŠ¨æ–‡æœ¬åµŒå…¥æ¨¡å‹å°è£…ç±»"""
    
    def __init__(self, api_key):
        self.api_key = api_key
        self.model = "BAAI/bge-m3"
        self.url = "https://api.siliconflow.cn/v1/embeddings"
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡æ–‡æ¡£åµŒå…¥"""
        embeddings = []
        for text in texts:
            result = self._embed_text(text)
            if result:
                embeddings.append(result)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """å•ä¸ªæŸ¥è¯¢æ–‡æœ¬åµŒå…¥"""
        return self._embed_text(text)
    
    def _embed_text(self, text: str) -> Optional[List[float]]:
        """è°ƒç”¨APIç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        payload = {
            "input": text,
            "encoding_format": "float",
            "model": self.model
        }
        headers = {
            "Authorization": self.api_key if self.api_key.startswith("Bearer ") else f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        try:
            response = requests.request("POST", self.url, json=payload, headers=headers)
            response.raise_for_status()
            result = response.json()
            return result["data"][0]["embedding"]
        except requests.exceptions.RequestException as e:
            st.error(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
            if hasattr(e.response, 'text'):
                st.error(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
            return None
        except Exception as e:
            st.error(f"å¤„ç†å“åº”æ—¶å‡ºé”™: {str(e)}")
            return None

    def __call__(self, input):
        """é€‚é… Chroma æ¥å£"""
        if isinstance(input, str):
            return self.embed_query(input)
        elif isinstance(input, list):
            return self.embed_documents(input)
        else:
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")

# èŠå¤©å†å²ç®¡ç†
# åˆå§‹åŒ–å’Œç®¡ç†èŠå¤©ä¼šè¯çŠ¶æ€
def init_session():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "retriever" not in st.session_state:
        st.session_state["retriever"] = None
    if "last_query" not in st.session_state:
        st.session_state["last_query"] = None

# åˆ›å»ºèŠå¤©æ¶ˆæ¯å†å²è®°å½•
msgs = StreamlitChatMessageHistory()

# è‡ªå®šä¹‰DeepSeekè¯­è¨€æ¨¡å‹
# å°è£…DeepSeek APIè°ƒç”¨ï¼Œæä¾›å¯¹è¯èƒ½åŠ›
class DeepSeekLLM:
    def __init__(self, api_key, model="deepseek-chat"):
        self.api_key = api_key
        self.model = model
        # ä½¿ç”¨OpenAI SDKè°ƒç”¨DeepSeek API
        self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
    
    def __call__(self, prompt):
        """è°ƒç”¨DeepSeek APIç”Ÿæˆå›å¤"""
        try:
            # ä¸ºæ–‡æ¡£é—®ç­”ä»»åŠ¡æä¾›æ˜ç¡®çš„ç³»ç»ŸæŒ‡ä»¤
            system_message = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚
æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­åŒ…å«ç­”æ¡ˆï¼Œè¯·åŸºäºæ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”ï¼Œä¸è¦æ·»åŠ é¢å¤–ä¿¡æ¯ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·å›ç­”"æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜æˆ‘æ— æ³•åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
ä¸è¦ç¼–é€ ä¸åœ¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯ã€‚
ä¿æŒå›ç­”ç®€æ´ã€å‡†ç¡®ã€æœ‰å¸®åŠ©ã€‚"""
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥æé«˜å›ç­”çš„ç¡®å®šæ€§
                max_tokens=1000,   # æ§åˆ¶å›ç­”çš„é•¿åº¦
                stream=False
            )
            return response.choices[0].message.content
        except Exception as e:
            st.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–å›å¤ã€‚"

# åˆå§‹åŒ–DeepSeekè¯­è¨€æ¨¡å‹
deepseek_api_key = "sk-ff6c3fa4fc4e453b92b7d023cd9efc4e"
llm = DeepSeekLLM(api_key=deepseek_api_key)

# å¤„ç†æ–‡æ¡£çš„å‡½æ•°
def process_documents(input_files: List[str], output_dir: str, save_to_local=False, local_path=None) -> Optional[Chroma]:
    """å¤„ç†æ–‡æ¡£å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“"""
    # ç¡®å®šæœ€ç»ˆçš„è¾“å‡ºç›®å½•
    final_output_dir = local_path if save_to_local and local_path else output_dir
    
    # åˆå§‹åŒ–ç¡…åŸºæµåŠ¨åµŒå…¥æ¨¡å‹
    embeddings = SiliconFlowEmbeddings(api_key="sk-nhyeljqothggnyzntjdzecdhivhvstzyqubhtafplbrxcjhi")
    
    # åŠ è½½æ–‡æ¡£
    loaded_docs = []
    for file_path in input_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
                loaded_docs.append(Document(page_content=text, metadata={"source": file_path}))
            st.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path}")
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    text = f.read()
                    loaded_docs.append(Document(page_content=text, metadata={"source": file_path}))
                st.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶(GBKç¼–ç ): {file_path}")
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {str(e)}")
                continue
    
    if not loaded_docs:
        st.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
        return None
    
    # æ–‡æœ¬åˆ†å‰²
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""]
    )
    splits = text_splitter.split_documents(loaded_docs)
    st.info(f"æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")
    
    try:
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        from langchain_chroma import Chroma
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(final_output_dir, exist_ok=True)
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        db = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=final_output_dir
        )
        
        st.success(f"å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼Œä¿å­˜åœ¨: {final_output_dir}")
        return db
        
    except Exception as e:
        st.error(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        return None

def load_vectordb(db_path: str) -> Optional[Chroma]:
    """åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“"""
    st.info(f"å°è¯•åŠ è½½å‘é‡æ•°æ®åº“: {db_path}")
    
    if not os.path.exists(db_path):
        st.error(f"å‘é‡æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {db_path}")
        return None
    
    try:
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = SiliconFlowEmbeddings(api_key="sk-nhyeljqothggnyzntjdzecdhivhvstzyqubhtafplbrxcjhi")
        
        # åŠ è½½å‘é‡æ•°æ®åº“
        from langchain_chroma import Chroma
        db = Chroma(
            persist_directory=db_path,
            embedding_function=embeddings
        )
        
        st.success("å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
        return db
        
    except Exception as e:
        st.error(f"åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        return None

def check_directory_writable(directory):
    """æ£€æŸ¥ç›®å½•æ˜¯å¦å¯å†™"""
    if not os.path.exists(directory):
        try:
            os.makedirs(directory, exist_ok=True)
            return True
        except Exception:
            return False
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰å†™æƒé™
        try:
            test_file = os.path.join(directory, ".write_test")
            with open(test_file, "w") as f:
                f.write("test")
            os.remove(test_file)
            return True
        except Exception:
            return False

def load_multiple_vectordbs(vectordb_dirs):
    """
    åŠ è½½å¤šä¸ªå‘é‡æ•°æ®åº“å¹¶åˆå¹¶ä¸ºä¸€ä¸ªæŸ¥è¯¢æ¥å£
    
    å‚æ•°:
    - vectordb_dirs: å‘é‡æ•°æ®åº“ç›®å½•åˆ—è¡¨
    
    è¿”å›:
    - combined_retriever: åˆå¹¶åçš„æ£€ç´¢å™¨ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    if not vectordb_dirs:
        st.warning("æœªé€‰æ‹©ä»»ä½•å‘é‡åº“")
        return None
        
    # è®°å½•æˆåŠŸåŠ è½½çš„å‘é‡åº“
    loaded_vectordbs = []
    
    # åŠ è½½æ¯ä¸ªå‘é‡åº“
    for db_dir in vectordb_dirs:
        try:
            st.info(f"æ­£åœ¨åŠ è½½å‘é‡åº“: {db_dir}")
            vectordb = load_vectordb(db_dir)
            if vectordb:
                loaded_vectordbs.append(vectordb)
                st.success(f"æˆåŠŸåŠ è½½å‘é‡åº“: {db_dir}")
            else:
                st.error(f"åŠ è½½å‘é‡åº“å¤±è´¥: {db_dir}")
        except Exception as e:
            st.error(f"åŠ è½½å‘é‡åº“ {db_dir} æ—¶å‡ºé”™: {str(e)}")
    
    if not loaded_vectordbs:
        st.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å‘é‡åº“")
        return None
    
    if len(loaded_vectordbs) == 1:
        # åªæœ‰ä¸€ä¸ªå‘é‡åº“ï¼Œç›´æ¥è¿”å›
        return loaded_vectordbs[0]
    else:
        # æœ‰å¤šä¸ªå‘é‡åº“ï¼Œåˆ›å»ºå¤šé‡æ£€ç´¢å™¨
        try:
            from langchain.retrievers import MergerRetriever
            
            # åˆ›å»ºæ£€ç´¢å™¨åˆ—è¡¨
            retrievers = []
            for i, vectordb in enumerate(loaded_vectordbs):
                retrievers.append(vectordb.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 2}
                ))
            
            # åˆå¹¶æ£€ç´¢å™¨
            merged_retriever = MergerRetriever(retrievers=retrievers)
            st.success(f"æˆåŠŸåˆå¹¶ {len(retrievers)} ä¸ªå‘é‡åº“ä¸ºç»Ÿä¸€æ£€ç´¢æ¥å£")
            return merged_retriever
        except Exception as e:
            st.error(f"åˆå¹¶æ£€ç´¢å™¨æ—¶å‡ºé”™: {str(e)}")
            # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå‘é‡åº“
            st.warning("åˆå¹¶å¤±è´¥ï¼Œå°†ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸåŠ è½½çš„å‘é‡åº“")
            return loaded_vectordbs[0]

# æŸ¥æ‰¾æœ¬åœ°å‘é‡åº“ç›®å½•
def find_vectordb_dirs(base_dir="/Users/yeziyin"):
    """
    åœ¨åŸºç¡€ç›®å½•ä¸‹æŸ¥æ‰¾å¯èƒ½çš„å‘é‡æ•°æ®åº“ç›®å½•
    
    å‚æ•°:
    - base_dir: åŸºç¡€ç›®å½•è·¯å¾„
    
    è¿”å›:
    - vectordb_dirs: å¯èƒ½çš„å‘é‡æ•°æ®åº“ç›®å½•åˆ—è¡¨
    """
    vectordb_dirs = []
    
    # é¦–å…ˆæ£€æŸ¥å¸¸ç”¨ä½ç½®
    common_dirs = [
        os.path.join(base_dir, "vectordb"),
        os.path.join(base_dir, "langchian", "langchain-rag", "vectordb"),
        os.path.join(base_dir, "langchian", "langchain-rag", "temp_vectordb"),
        "vectordb",
        "temp_vectordb"
    ]
    
    # æ·»åŠ ç¡®å®å­˜åœ¨å¹¶åŒ…å«Chromaæ•°æ®åº“æ–‡ä»¶çš„ç›®å½•
    for dir_path in common_dirs:
        if os.path.exists(dir_path) and os.path.isdir(dir_path):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Chromaæ•°æ®åº“æ–‡ä»¶
            if os.path.exists(os.path.join(dir_path, "chroma.sqlite3")):
                vectordb_dirs.append(dir_path)
    
    return vectordb_dirs

# ä¸»åº”ç”¨é€»è¾‘
def main():
    # è®¾ç½®é¡µé¢æ ‡é¢˜
    st.title("æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ å†…å®¹
    with st.sidebar:
        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        st.info("ç³»ç»ŸçŠ¶æ€: æ­£å¸¸")
        
        # æ·»åŠ æ¸…é™¤æŒ‰é’®
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰è®°å½•", help="æ¸…é™¤èŠå¤©è®°å½•å’ŒåŠ è½½çš„æ–‡æ¡£"):
            # æ¸…é™¤æ‰€æœ‰ä¼šè¯çŠ¶æ€ï¼Œä½†ä¸ä½¿ç”¨experimental_rerun
            st.session_state["messages"] = []
            st.session_state["retriever"] = None
            st.session_state["last_query"] = None
            if "last_context" in st.session_state:
                del st.session_state["last_context"]
            st.success("å·²æ¸…é™¤æ‰€æœ‰è®°å½•ï¼")
            # è¿”å›ï¼Œé¿å…ç»§ç»­æ‰§è¡Œä¸‹é¢çš„ä»£ç 
            return
        
        # è°ƒè¯•æ¨¡å¼å¼€å…³
        debug_mode = st.checkbox("å¼€å¯è°ƒè¯•æ¨¡å¼", value=False, help="æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†è¿‡ç¨‹")
        if debug_mode:
            st.session_state["debug_mode"] = True
        else:
            st.session_state["debug_mode"] = False
    
    # ä¸»å†…å®¹åŒº
    # å¼ºåˆ¶æ˜¾ç¤ºä¸Šä¼ åŠŸèƒ½
    st.markdown("## é€‰æ‹©æ“ä½œæ¨¡å¼")
    operation_mode = st.radio(
        "æ“ä½œæ–¹å¼:",
        ["ä¸Šä¼ æ–°æ–‡æ¡£", "åŠ è½½æœ¬åœ°å‘é‡åº“"]
    )
    
    retriever = None
    
    if operation_mode == "ä¸Šä¼ æ–°æ–‡æ¡£":
        st.markdown("### ğŸ“ ä¸Šä¼ æ–‡ä»¶åŒºåŸŸ")
        
        # ç›´æ¥åœ¨ä¸»ç•Œé¢æ˜¾ç¤ºä¸Šä¼ ç»„ä»¶
        uploaded_files = st.file_uploader(
            "é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡æ¡£", 
            accept_multiple_files=True, 
            type=["txt", "md", "pdf"],
            help="æ”¯æŒtxtã€mdå’Œpdfæ ¼å¼çš„æ–‡æ¡£"
        )
        
        # æ˜¾ç¤ºä¸Šä¼ çŠ¶æ€
        if uploaded_files:
            st.success(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            for f in uploaded_files:
                st.write(f"- {f.name} ({f.size} å­—èŠ‚)")
        else:
            st.warning("å°šæœªé€‰æ‹©ä»»ä½•æ–‡ä»¶ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹'Browse files'æŒ‰é’®é€‰æ‹©æ–‡ä»¶")
        
        # æ˜¯å¦ä¿å­˜åˆ°æœ¬åœ°
        save_locally = st.checkbox("ä¿å­˜åˆ°æœ¬åœ°å‘é‡åº“")
        local_path = None
        
        if save_locally:
            local_path = st.text_input("æœ¬åœ°å‘é‡åº“ä¿å­˜è·¯å¾„:", value="/Users/yeziyin/vectordb")
            
            # æ£€æŸ¥ç›®å½•æƒé™
            if local_path:
                if check_directory_writable(local_path):
                    st.success(f"âœ… ç›®å½• {local_path} å¯å†™å…¥")
                else:
                    st.error(f"âŒ æ— æ³•å†™å…¥ç›®å½• {local_path}ï¼Œè¯·æ£€æŸ¥æƒé™æˆ–é€‰æ‹©å…¶ä»–ç›®å½•")
            
        # å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
        process_button = st.button("ğŸ“„ å¤„ç†æ–‡æ¡£")
        if process_button:
            if not uploaded_files:
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶å†å¤„ç†")
            else:
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                    temp_dir = "temp_docs"
                    if not check_directory_writable(temp_dir):
                        st.error(f"ä¸´æ—¶ç›®å½• {temp_dir} ä¸å¯å†™å…¥ï¼Œè¯·æ£€æŸ¥åº”ç”¨æƒé™")
                        return
                        
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    for filename in os.listdir(temp_dir):
                        file_path = os.path.join(temp_dir, filename)
                        try:
                            if os.path.isfile(file_path):
                                os.unlink(file_path)
                        except Exception as e:
                            st.error(f"åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                    
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    file_paths = []
                    for uploaded_file in uploaded_files:
                        try:
                            file_path = os.path.join(temp_dir, uploaded_file.name)
                            with open(file_path, "wb") as f:
                                f.write(uploaded_file.getbuffer())
                            file_paths.append(file_path)
                            st.info(f"å·²ä¿å­˜æ–‡ä»¶: {file_path}")
                        except Exception as e:
                            st.error(f"ä¿å­˜æ–‡ä»¶ {uploaded_file.name} æ—¶å‡ºé”™: {str(e)}")
                    
                    if not file_paths:
                        st.error("æ²¡æœ‰æˆåŠŸä¿å­˜ä»»ä½•æ–‡ä»¶ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                        return
                        
                    # åˆ›å»ºä¸´æ—¶å‘é‡æ•°æ®åº“ç›®å½•
                    vectordb_dir = "temp_vectordb"
                    
                    # å¤„ç†æ–‡æ¡£
                    retriever = process_documents(
                        file_paths, 
                        vectordb_dir,
                        save_to_local=save_locally,
                        local_path=local_path
                    )
                    
                    if retriever:
                        st.session_state.retriever = retriever
                        st.success("æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå¯ä»¥å¼€å§‹æé—®")
                    else:
                        st.error("å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
    
    elif operation_mode == "åŠ è½½æœ¬åœ°å‘é‡åº“":
        # æŸ¥æ‰¾å¯èƒ½çš„å‘é‡åº“ç›®å½•
        vectordb_dirs = find_vectordb_dirs()
        
        # æä¾›è‡ªå®šä¹‰è·¯å¾„è¾“å…¥
        custom_path = st.text_input("æ·»åŠ æœ¬åœ°å‘é‡åº“è·¯å¾„:", value="/Users/yeziyin/vectordb")
        if custom_path and custom_path not in vectordb_dirs and os.path.exists(custom_path):
            vectordb_dirs.append(custom_path)
        
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å‘é‡åº“ç›®å½•æ—¶çš„æç¤º
        if not vectordb_dirs:
            st.warning("æœªæ‰¾åˆ°ä»»ä½•å‘é‡åº“ç›®å½•ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥è·¯å¾„")
        else:
            st.success(f"æ‰¾åˆ° {len(vectordb_dirs)} ä¸ªå¯èƒ½çš„å‘é‡åº“ç›®å½•")
        
        # æ˜¾ç¤ºæ‰€æœ‰å¯é€‰å‘é‡åº“
        options = []
        for db_dir in vectordb_dirs:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„Chromaæ•°æ®åº“
            has_db_file = os.path.exists(os.path.join(db_dir, "chroma.sqlite3"))
            status = "âœ…" if has_db_file else "â“"
            options.append(f"{status} {db_dir}")
        
        # å¤šé€‰æ¡†ç»„ä»¶
        if options:
            st.write("### é€‰æ‹©è¦ä½¿ç”¨çš„å‘é‡åº“:")
            selected_options = st.multiselect(
                "å¯ç”¨çš„å‘é‡åº“åˆ—è¡¨",
                options,
                default=[options[0]] if options else None,
                help="é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªå‘é‡åº“ä½œä¸ºçŸ¥è¯†æ¥æº"
            )
            
            # æå–è·¯å¾„
            selected_dirs = [opt.split(" ", 1)[1] for opt in selected_options]
        else:
            selected_dirs = []
            if custom_path and os.path.exists(custom_path):
                selected_dirs = [custom_path]
        
        # åŠ è½½å‘é‡åº“
        if st.button("åŠ è½½é€‰ä¸­çš„å‘é‡åº“"):
            # æ¸…é™¤å¯èƒ½çš„æ—§çŠ¶æ€
            if "retriever" in st.session_state:
                del st.session_state["retriever"]
            
            with st.spinner("æ­£åœ¨åŠ è½½å‘é‡åº“..."):
                if selected_dirs:
                    # åŠ è½½å¤šä¸ªå‘é‡åº“
                    retriever = load_multiple_vectordbs(selected_dirs)
                    
                    if retriever:
                        st.session_state.retriever = retriever
                        st.success("âœ… å‘é‡åº“åŠ è½½æˆåŠŸï¼Œå¯ä»¥å¼€å§‹æé—®")
                    else:
                        st.error("âŒ åŠ è½½å‘é‡åº“å¤±è´¥")
                else:
                    st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå‘é‡åº“")

    # ä½¿ç”¨ä¸€ä¸ªå®¹å™¨æ¥ç»„ç»‡é—®ç­”åŒºåŸŸ
    qa_container = st.container()
    
    with qa_container:
        # ä¸»ç•Œé¢ï¼šæé—®éƒ¨åˆ†
        if 'retriever' in st.session_state:
            retriever = st.session_state.retriever
            st.success("ğŸ“š å‘é‡æ•°æ®åº“å·²åŠ è½½")
            
            # ç”¨æˆ·æé—®åŒºåŸŸ
            st.markdown("---")
            st.header("å‘æ–‡æ¡£æé—®")
            user_query = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜:")
            
            # æ·»åŠ æŸ¥è¯¢çŠ¶æ€ç®¡ç†ï¼Œé¿å…é‡å¤å¤„ç†
            if user_query:
                # åˆ›å»ºä¸¤ä¸ªåˆ—æ¥åˆ†åˆ«æ˜¾ç¤ºç»“æœå’Œè°ƒè¯•ä¿¡æ¯
                if st.session_state.get("debug_mode", False):
                    # è°ƒè¯•æ¨¡å¼ä¸‹ä½¿ç”¨ä¸€åˆ—
                    result_col = st.container()
                else:
                    # æ™®é€šæ¨¡å¼ä¸‹ä½¿ç”¨ä¸¤åˆ—
                    result_col, debug_col = st.columns([3, 1])
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æŸ¥è¯¢
                is_new_query = user_query != st.session_state.get("last_query")
                
                if is_new_query:
                    # å¤„ç†ç”¨æˆ·æŸ¥è¯¢
                    with st.spinner("æ­£åœ¨æ€è€ƒ..."):
                        # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œç›´æ¥åœ¨é¡µé¢ä¸Šæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        if st.session_state.get("debug_mode", False):
                            response_text, context = process_query(user_query, retriever)
                        else:
                            # éè°ƒè¯•æ¨¡å¼ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªæ•è·è°ƒè¯•è¾“å‡ºçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                            with st.echo() as debug_output:
                                # ä¸´æ—¶å…³é—­st.writeè¾“å‡º
                                def silent_write(*args, **kwargs):
                                    pass
                                original_write = st.write
                                st.write = silent_write
                                
                                try:
                                    response_text, context = process_query(user_query, retriever)
                                finally:
                                    # æ¢å¤st.write
                                    st.write = original_write
                    
                    # ä¿å­˜æŸ¥è¯¢çŠ¶æ€å’ŒèŠå¤©å†å²
                    st.session_state["last_query"] = user_query
                    st.session_state["messages"].append({"role": "user", "content": user_query})
                    st.session_state["messages"].append({"role": "assistant", "content": response_text})
                    st.session_state["last_context"] = context
                else:
                    # ä½¿ç”¨ä¿å­˜çš„å›ç­”
                    response_text = st.session_state["messages"][-1]["content"]
                    context = st.session_state.get("last_context", "")
                
                # æ˜¾ç¤ºç»“æœ
                with result_col:
                    st.subheader("å›ç­”:")
                    st.markdown(response_text)
                    
                    # å¯æŠ˜å çš„å‚è€ƒæ–‡æ¡£åŒºåŸŸ
                    with st.expander("å‚è€ƒæ–‡æ¡£å†…å®¹"):
                        st.markdown(context)
        
            # èŠå¤©å†å²æ˜¾ç¤º
            if "messages" in st.session_state and st.session_state["messages"]:
                st.markdown("---")
                st.subheader("èŠå¤©å†å²")
                for i, msg in enumerate(st.session_state["messages"]):
                    if msg["role"] == "user":
                        st.markdown(f"**é—®é¢˜**: {msg['content']}")
                    else:
                        st.markdown(f"**å›ç­”**: {msg['content']}")
                
                # é™¤äº†æœ€åä¸€æ¡æ¶ˆæ¯å¤–ï¼Œæ¯æ¡æ¶ˆæ¯åæ·»åŠ åˆ†éš”çº¿
                if i < len(st.session_state["messages"]) - 1:
                    st.markdown("---")
                    
# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("æ–‡æ¡£é—®ç­”ç³»ç»Ÿ | åŸºäºLangChainå’ŒDeepSeek")

# å¤„ç†ç”¨æˆ·æŸ¥è¯¢
def process_query(query_text, retriever):
    """
    å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶è¿”å›å›ç­”å’Œä¸Šä¸‹æ–‡
    
    å‚æ•°:
    - query_text: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
    - retriever: æ£€ç´¢å™¨å¯¹è±¡ï¼Œå¯ä»¥æ˜¯å‘é‡æ•°æ®åº“æˆ–åˆå¹¶æ£€ç´¢å™¨
    
    è¿”å›:
    - response: æ¨¡å‹å›ç­”
    - context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    """
    if not query_text:
        return "è¯·è¾“å…¥é—®é¢˜", ""
        
    try:
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        st.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {query_text}")
        
        # æ£€æŸ¥æ£€ç´¢å™¨ç±»å‹
        retriever_type = type(retriever).__name__
        st.write(f"æ£€ç´¢å™¨ç±»å‹: {retriever_type}")
        
        # è·å–ç›¸å…³æ–‡æ¡£
        relevant_docs = []
        
        # æ ¹æ®æ£€ç´¢å™¨ç±»å‹ä½¿ç”¨ä¸åŒçš„æ£€ç´¢æ–¹æ³•
        if hasattr(retriever, "as_retriever") and callable(retriever.as_retriever):
            # æ˜¯å‘é‡æ•°æ®åº“å¯¹è±¡ï¼Œéœ€è¦è½¬æ¢ä¸ºæ£€ç´¢å™¨
            st.write("æ£€æµ‹åˆ°å‘é‡æ•°æ®åº“å¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ£€ç´¢å™¨...")
            try:
                # å°è¯•MMRæ£€ç´¢
                st.write("å°è¯•MMRæ£€ç´¢...")
                mmr_docs = retriever.max_marginal_relevance_search(
                    query_text, 
                    k=5,
                    fetch_k=10
                )
                if mmr_docs:
                    st.write(f"MMRæ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(mmr_docs)} ä¸ªæ–‡æ¡£")
                    relevant_docs = mmr_docs
            except Exception as e:
                st.write(f"MMRæ£€ç´¢å¤±è´¥: {str(e)}")
                try:
                    # å°è¯•ç›¸ä¼¼åº¦æ£€ç´¢
                    st.write("å°è¯•ç›¸ä¼¼åº¦æ£€ç´¢...")
                    sim_docs = retriever.similarity_search(
                        query_text, 
                        k=5
                    )
                    if sim_docs:
                        st.write(f"ç›¸ä¼¼åº¦æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(sim_docs)} ä¸ªæ–‡æ¡£")
                        relevant_docs = sim_docs
                except Exception as e2:
                    st.write(f"ç›¸ä¼¼åº¦æ£€ç´¢å¤±è´¥: {str(e2)}")
        elif hasattr(retriever, "get_relevant_documents") and callable(retriever.get_relevant_documents):
            # å·²ç»æ˜¯æ£€ç´¢å™¨å¯¹è±¡
            st.write("ä½¿ç”¨æ£€ç´¢å™¨ç›´æ¥è·å–æ–‡æ¡£...")
            try:
                docs = retriever.get_relevant_documents(query_text)
                st.write(f"æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                relevant_docs = docs
            except Exception as e:
                st.write(f"æ£€ç´¢å¤±è´¥: {str(e)}")
                
                # å¦‚æœæ˜¯åˆå¹¶æ£€ç´¢å™¨ï¼Œå°è¯•å•ç‹¬è°ƒç”¨æ¯ä¸ªæ£€ç´¢å™¨
                if hasattr(retriever, "retrievers"):
                    st.write("å°è¯•å•ç‹¬è°ƒç”¨æ¯ä¸ªæ£€ç´¢å™¨...")
                    for i, sub_retriever in enumerate(retriever.retrievers):
                        try:
                            sub_docs = sub_retriever.get_relevant_documents(query_text)
                            st.write(f"æ£€ç´¢å™¨ {i+1} è¿”å› {len(sub_docs)} ä¸ªæ–‡æ¡£")
                            relevant_docs.extend(sub_docs)
                        except Exception as sub_e:
                            st.write(f"æ£€ç´¢å™¨ {i+1} å¤±è´¥: {str(sub_e)}")
        else:
            # å°è¯•ç›´æ¥ä½¿ç”¨getæ–¹æ³•
            st.write("å°è¯•ç›´æ¥è·å–æ–‡æ¡£é›†åˆ...")
            try:
                if hasattr(retriever, "get") and callable(retriever.get):
                    collection_docs = retriever.get()
                    if collection_docs and 'documents' in collection_docs:
                        st.write(f"è·å–åˆ° {len(collection_docs['documents'])} ä¸ªæ–‡æ¡£")
                        from langchain_core.documents import Document
                        relevant_docs = [
                            Document(
                                page_content=doc,
                                metadata=meta if meta else {}
                            ) 
                            for doc, meta in zip(
                                collection_docs['documents'][:5], 
                                collection_docs.get('metadatas', [{}] * len(collection_docs['documents']))[:5]
                            )
                        ]
            except Exception as e:
                st.write(f"ç›´æ¥è·å–æ–‡æ¡£å¤±è´¥: {str(e)}")
                
        # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
        if not relevant_docs:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”é—®é¢˜")
            return "æˆ‘æ— æ³•ä»æ–‡æ¡£ä¸­æ‰¾åˆ°è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆã€‚å¯èƒ½æ–‡æ¡£ä¸­ä¸åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œæˆ–æ•°æ®åº“è®¿é—®å‡ºç°é—®é¢˜ã€‚", ""
        
        # å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œæ’åºå’Œå»é‡
        try:
            # å»æ‰å®Œå…¨ç›¸åŒçš„æ–‡æ¡£
            unique_content = set()
            filtered_docs = []
            for doc in relevant_docs:
                if doc.page_content not in unique_content:
                    unique_content.add(doc.page_content)
                    filtered_docs.append(doc)
            
            relevant_docs = filtered_docs[:5]  # é™åˆ¶æœ€å¤š5ä¸ªæ–‡æ¡£
            st.write(f"è¿‡æ»¤åå‰©ä½™ {len(relevant_docs)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            st.write(f"æ–‡æ¡£å»é‡å¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
        st.write("å·²æ£€ç´¢åˆ°ä»¥ä¸‹æ–‡æ¡£:")
        for i, doc in enumerate(relevant_docs[:3]):
            st.write(f"æ–‡æ¡£ {i+1}: {doc.page_content[:100]}...")
        
        # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ç»„åˆæˆä¸Šä¸‹æ–‡
        context = "\n\n".join([f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{doc.page_content}" for i, doc in enumerate(relevant_docs)])
        
        # æ„å»ºæ›´å¼ºçš„æç¤ºæ¨¡æ¿
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœæ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•ä»æ–‡æ¡£ä¸­æ‰¾åˆ°è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆ"ã€‚
ä¸è¦ç¼–é€ ä¿¡æ¯ï¼Œåªä½¿ç”¨æä¾›çš„å†…å®¹ä½œç­”ã€‚
å…ˆåˆ†ææ–‡æ¡£å†…å®¹ä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼Œæ•´åˆç›¸å…³ä¿¡æ¯ï¼Œç„¶åç»™å‡ºå…·ä½“ç­”æ¡ˆã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query_text}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®ã€æœ‰æ¡ç†çš„å›ç­”ï¼š"""

        # å‘é€åˆ°DeepSeekæ¨¡å‹è·å–å›ç­”
        st.write("å‘é€è¯·æ±‚åˆ°DeepSeekæ¨¡å‹...")
        api_key = "sk-ff6c3fa4fc4e453b92b7d023cd9efc4e"
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œæ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œæé«˜å›ç­”çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        st.write("æ¨¡å‹å·²è¿”å›å›ç­”")
        return answer, context
        
    except Exception as e:
        log_error("å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™", e)
        return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯ã€‚", ""

# ç¡®ä¿ä¸»å‡½æ•°åœ¨å¯åŠ¨æ—¶æ‰§è¡Œ
if __name__ == "__main__":
    init_session()
    main()
